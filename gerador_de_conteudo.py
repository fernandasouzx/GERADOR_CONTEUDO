# -*- coding: utf-8 -*-
"""gerador_de_conteudo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AIFlBKZDzHRyz8Fd3aqIu530-khpiKa0

## **TRABALHO FINAL DA MATERIA DE DEEP LEARNING**
### ALUNA: FERNANDA LIMA DE SOUZA
RGA: 202311722012
"""

#@title 1. Configuração Completa do Ambiente

print("Iniciando configuração do ambiente...")

# Atualizar PIP, setuptools e packaging (fixando packaging para compatibilidade)
print("\n Atualizando pip, setuptools e packaging...")
!python -m pip install --upgrade pip setuptools "packaging>=23.2,<25" -q

# PASSO 0: Atualizar PIP
print("\n Atualizando pip...")
!python -m pip install --upgrade pip -q

# PASSO 1.A: Desinstalação Limpa e Agressiva
print("\n[PASSO 1.A] Desinstalando bibliotecas existentes para um ambiente limpo...")
!pip uninstall -y numpy torch torchvision torchaudio xformers bitsandbytes transformers accelerate diffusers huggingface_hub sentencepiece ipywidgets invisible_watermark mediapy torchtune tensorflow fastai sentence-transformers peft thinc spacy tensorboardX tensorboard numba
!pip cache purge
print("Desinstalação concluída.")

# PASSO 1.B: Instalar uma versão recente e compatível do NumPy 2.0.x
NUMPY_SPEC = "numpy>=2.0.0,<2.1.0"
EXPECTED_NUMPY_START = "2.0."
print(f"\n[PASSO 1.B] Instalando {NUMPY_SPEC} com --force-reinstall...")
!pip install "{NUMPY_SPEC}" --force-reinstall --no-cache-dir -q

# PASSO 1.C: Instalar PyTorch
PYTORCH_VERSION = "2.3.1"
TORCHVISION_VERSION = "0.18.1"
TORCHAUDIO_VERSION = "2.3.1"
print(f"\n[PASSO 1.C] Instalando PyTorch {PYTORCH_VERSION}, torchvision {TORCHVISION_VERSION}, torchaudio {TORCHAUDIO_VERSION} (CUDA 12.1)...")
!pip install torch=={PYTORCH_VERSION}+cu121 torchvision=={TORCHVISION_VERSION}+cu121 torchaudio=={TORCHAUDIO_VERSION}+cu121 --index-url https://download.pytorch.org/whl/cu121 -q

# PASSO 1.D: Instalar outras dependências principais
print(f"\n[PASSO 1.D] Instalando outras dependências principais...")

# Transformers (requer huggingface-hub >=0.23.0)
TRANSFORMERS_VERSION = "4.41.2"
print(f"Instalando transformers=={TRANSFORMERS_VERSION}...")
!pip install transformers=={TRANSFORMERS_VERSION} -q # Isso pode puxar uma versão recente do huggingface_hub

# Diffusers (vamos tentar uma versão mais recente compatível com huggingface-hub >= 0.23.0)
# DIFFUSERS_VERSION = "0.29.0" # Ou 0.30.0, 0.31.0, 0.33.1
DIFFUSERS_VERSION = "0.29.0" # Começar com esta
print(f"Instalando diffusers=={DIFFUSERS_VERSION}...")
!pip install diffusers=={DIFFUSERS_VERSION} -q

# Accelerate
ACCELERATE_VERSION = "0.30.1"
print(f"Instalando accelerate=={ACCELERATE_VERSION}...")
!pip install accelerate=={ACCELERATE_VERSION} -q

# BitsAndBytes
BITSANDBYTES_VERSION = "0.43.2"
print(f"Instalando bitsandbytes=={BITSANDBYTES_VERSION}...")
!pip install bitsandbytes=={BITSANDBYTES_VERSION} -q

# Outras dependências
print("Instalando dependências secundárias...")
!pip install sentencepiece==0.2.0 -q
!pip install ipywidgets==7.7.1 -q
!pip install invisible_watermark mediapy -q

# PASSO 1.D.FINAL: Re-confirmar/Reinstalar NumPy para garantir que é a última palavra
print(f"\n[PASSO 1.D.FINAL] Re-confirmando NumPy versão {EXPECTED_NUMPY_START}x...")
!pip install "{NUMPY_SPEC}" --force-reinstall --no-cache-dir -q

# PASSO 1.E: Verificar consistência
print("\n[PASSO 1.E] Verificando consistência do ambiente com pip check...")
!pip check

print("\n*********************************************************************")
print("INSTALAÇÃO DO AMBIENTE CONCLUÍDA (v5 - NumPy 2.x).")
print("As bibliotecas foram instaladas.")
print("Se ocorrerem erros inesperados de importação ou versão, um 'Reiniciar Ambiente de Execução' pode ser necessário.")
print("*********************************************************************")

# Verificações finais na Célula 1 para diagnóstico rápido
print("\n--- VERIFICAÇÕES FINAIS DE VERSÃO NA CÉLULA 1 ---")
print("Versões listadas pelo pip:")
!pip list | grep -E "numpy|torch|transformers|accelerate|diffusers|bitsandbytes|huggingface-hub|sentencepiece|ipywidgets|packaging"

print("\nImportando e verificando versões principais via Python:")
try:
    import site
    import importlib
    importlib.reload(site)

    import numpy
    print(f"  NumPy importado (na Célula 1): {numpy.__version__} (Esperado: começando com '{EXPECTED_NUMPY_START}')")
    assert numpy.__version__.startswith(EXPECTED_NUMPY_START), f"NumPy importado ({numpy.__version__}) não começa com '{EXPECTED_NUMPY_START}'."

    import torch
    print(f"  PyTorch importado: {torch.__version__} (Esperado: {PYTORCH_VERSION}+cu121)")
    assert torch.__version__.startswith(PYTORCH_VERSION) and "+cu121" in torch.__version__, f"PyTorch não é {PYTORCH_VERSION}+cu121!"

    from packaging.version import parse as parse_version

    # Importar huggingface_hub e verificar sua versão (deve ser >=0.23.0)
    import huggingface_hub
    EXPECTED_HF_HUB_MIN_VERSION_STR = "0.23.0"
    print(f"  Hugging Face Hub importado: {huggingface_hub.__version__} (Esperado: >={EXPECTED_HF_HUB_MIN_VERSION_STR})")
    assert parse_version(huggingface_hub.__version__) >= parse_version(EXPECTED_HF_HUB_MIN_VERSION_STR), f"Hugging Face Hub versão ({huggingface_hub.__version__}) é menor que {EXPECTED_HF_HUB_MIN_VERSION_STR}!"

    import transformers
    print(f"  Transformers importado: {transformers.__version__} (Esperado: {TRANSFORMERS_VERSION})")
    assert parse_version(transformers.__version__) >= parse_version(TRANSFORMERS_VERSION), f"Transformers versão baixa!"

    import accelerate
    print(f"  Accelerate importado: {accelerate.__version__} (Esperado: {ACCELERATE_VERSION})")
    assert parse_version(accelerate.__version__) >= parse_version(ACCELERATE_VERSION), f"Accelerate versão baixa!"

    import diffusers
    print(f"  Diffusers importado: {diffusers.__version__} (Esperado: {DIFFUSERS_VERSION})")
    assert parse_version(diffusers.__version__) >= parse_version(DIFFUSERS_VERSION), f"Diffusers versão baixa!"

    import bitsandbytes
    print(f"  Bitsandbytes importado: {bitsandbytes.__version__} (Esperado: {BITSANDBYTES_VERSION})")
    assert parse_version(bitsandbytes.__version__) >= parse_version(BITSANDBYTES_VERSION), f"Bitsandbytes versão baixa!"


    print("  SUCESSO: Todas as importações e verificações de versão principais passaram na Célula 1!")
except Exception as e:
    print(f"  ERRO nas importações básicas ou asserts de versão pós-instalação: {e}")
    import traceback
    traceback.print_exc()

#@title 2. Importar Bibliotecas e Verificar Ambiente

import sys
import os
import importlib
from packaging.version import parse as parse_version # Usar para comparações de versão robustas

print(f"Caminhos do sistema (sys.path):\n{sys.path}\n")
print(f"Python version: {sys.version}\n")

environment_ok = False; cuda_available = False; device = "cpu"
pytorch_ok = False; numpy_ok = False; huggingface_hub_ok = False
transformers_ok = False; diffusers_ok = False; accelerate_ok = False
bitsandbytes_ok = False; ipywidgets_ok = False

# --- VERSÕES ESPERADAS APÓS A CÉLULA 1 MODIFICADA ---
# Ajustar estas conforme as versões exatas intaladas ou esperadas.
# Usa as versões da Célula 1 como referência.
EXPECTED_PYTORCH_VERSION_START = "2.3.1" # Célula 1 instala PYTORCH_VERSION
EXPECTED_NUMPY_VERSION_MIN = "2.0.0"    # Célula 1 tenta NUMPY_VERSION (2.0.3), mas esperamos pelo menos 2.x
EXPECTED_HF_HUB_MIN_VERSION = "0.22.0" # Célula 1 instala huggingface_hub>=0.22.0
EXPECTED_TRANSFORMERS_MIN_VERSION = "4.41.2" # Célula 1 instala TRANSFORMERS_VERSION
EXPECTED_DIFFUSERS_MIN_VERSION = "0.27.2" # Célula 1 instala DIFFUSERS_VERSION
EXPECTED_ACCELERATE_MIN_VERSION = "0.30.1" # Célula 1 instala ACCELERATE_VERSION
EXPECTED_BITSANDBYTES_VERSION = "0.43.2" # Célula 1 instala BITSANDBYTES_VERSION
EXPECTED_IPYWIDGETS_VERSION = "7.7.1"   # Célula 1 instala ipywidgets==7.7.1
# ----------------------------------------------------

try:
    print("Verificando PyTorch...")
    import torch
    print(f"  SUCESSO! PyTorch: {torch.__version__}")
    assert torch.__version__.startswith(EXPECTED_PYTORCH_VERSION_START) and "+cu121" in torch.__version__, \
        f"PyTorch: Esperado ~{EXPECTED_PYTORCH_VERSION_START}+cu121, Instalado: {torch.__version__}"
    pytorch_ok = True
    if torch.cuda.is_available():
        print(f"  GPU detectada: {torch.cuda.get_device_name(0)}, CUDA: {torch.version.cuda}")
        cuda_available = True; device = "cuda"
    else:
        print("  ALERTA: CUDA NÃO disponível. Usando CPU."); cuda_available = False

    print("\nVerificando NumPy...")
    import numpy
    print(f"  SUCESSO! NumPy: {numpy.__version__}")
    assert parse_version(numpy.__version__) >= parse_version(EXPECTED_NUMPY_VERSION_MIN) and numpy.__version__.startswith("2."), \
        f"NumPy: Esperado >={EXPECTED_NUMPY_VERSION_MIN} (versão 2.x), Instalado: {numpy.__version__}"
    numpy_ok = True

    print("\nVerificando Hugging Face Hub...")
    import huggingface_hub
    print(f"  SUCESSO! huggingface_hub: {huggingface_hub.__version__}")
    assert parse_version(huggingface_hub.__version__) >= parse_version(EXPECTED_HF_HUB_MIN_VERSION), \
        f"huggingface_hub: Esperado >={EXPECTED_HF_HUB_MIN_VERSION}, Instalado: {huggingface_hub.__version__}"
    huggingface_hub_ok = True

    print("\nVerificando Transformers...")
    import transformers
    print(f"  SUCESSO! Transformers: {transformers.__version__}")
    assert parse_version(transformers.__version__) >= parse_version(EXPECTED_TRANSFORMERS_MIN_VERSION), \
        f"Transformers: Esperado >={EXPECTED_TRANSFORMERS_MIN_VERSION}, Instalado: {transformers.__version__}"
    transformers_ok = True

    print("\nVerificando Diffusers...")
    import diffusers
    print(f"  SUCESSO! Diffusers: {diffusers.__version__}")
    assert parse_version(diffusers.__version__) >= parse_version(EXPECTED_DIFFUSERS_MIN_VERSION), \
        f"Diffusers: Esperado >={EXPECTED_DIFFUSERS_MIN_VERSION}, Instalado: {diffusers.__version__}"
    diffusers_ok = True

    print("\nVerificando Accelerate...")
    import accelerate
    print(f"  SUCESSO! Accelerate: {accelerate.__version__}")
    assert parse_version(accelerate.__version__) >= parse_version(EXPECTED_ACCELERATE_MIN_VERSION), \
        f"Accelerate: Esperado >={EXPECTED_ACCELERATE_MIN_VERSION}, Instalado: {accelerate.__version__}"
    accelerate_ok = True

    print("\nVerificando bitsandbytes...")
    import bitsandbytes
    print(f"  SUCESSO! bitsandbytes: {bitsandbytes.__version__}")
    assert parse_version(bitsandbytes.__version__) == parse_version(EXPECTED_BITSANDBYTES_VERSION), \
        f"bitsandbytes: Esperado {EXPECTED_BITSANDBYTES_VERSION}, Instalado: {bitsandbytes.__version__}"
    bitsandbytes_ok = True

    print("\nVerificando ipywidgets...")
    import ipywidgets
    print(f"  SUCESSO! ipywidgets: {ipywidgets.__version__}")
    assert parse_version(ipywidgets.__version__) == parse_version(EXPECTED_IPYWIDGETS_VERSION), \
        f"ipywidgets: Esperado {EXPECTED_IPYWIDGETS_VERSION}, Instalado: {ipywidgets.__version__}"
    ipywidgets_ok = True

    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
    from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline
    from IPython.display import display, Image as IPImage, HTML, clear_output
    import re; import gc; import time

    print("\n--- VERIFICAÇÃO DE BIBLIOTECAS CONCLUÍDA ---")
    if all([pytorch_ok, numpy_ok, huggingface_hub_ok, transformers_ok, diffusers_ok, accelerate_ok, bitsandbytes_ok, ipywidgets_ok]):
        environment_ok = True
        print("Todas as bibliotecas principais parecem estar corretas e importadas!")
        print(f"Dispositivo configurado: {device}" + (f" ({torch.cuda.get_device_name(0)})" if cuda_available else ""))
    else:
        print("ATENÇÃO: Uma ou mais bibliotecas principais não estão corretas. Revise os asserts e logs.")

except ImportError as e: print(f"  ERRO CRÍTICO DE IMPORTAÇÃO: {e}.")
except AssertionError as e: print(f"  ERRO DE VERIFICAÇÃO DE VERSÃO: {e}")
except Exception as e: print(f"  ERRO INESPERADO DURANTE A VERIFICAÇÃO: {e}")

if not environment_ok:
    print("\n!!! ATENÇÃO: A verificação do ambiente falhou. NÃO PROSSIGA até corrigir. !!!")
else:
    print("\nAmbiente verificado com sucesso. Prossiga para carregar os modelos (Célula 3).")

#@title 3. Carregar Modelos (LLM para Texto e SDXL para Imagem) - Com Llama 3 Instruct
import torch
import gc
from huggingface_hub import notebook_login

# Flags de sucesso
llm_loaded_successfully = False
sdxl_loaded_successfully = False
base_pipe = None
refiner_pipe = None
llm_model = None
llm_tokenizer = None

USE_LLM_4BIT_QUANTIZATION = True # Manter True para rodar com Llama 8B e SDXL na T4

if 'environment_ok' in globals() and environment_ok:
    print("Ambiente OK. Iniciando carregamento dos modelos...")

    print("\nInfo: Fazendo login no Hugging Face Hub (necessário para modelos Llama)...")
    notebook_login() # Executa o login. Forneça seu token quando solicitado.
    print("Info: Login no Hugging Face Hub concluído.\n")

    # --- Função para carregar LLM (permite recarregamento pela Célula 5) ---
    def load_llm():
        global llm_model, llm_tokenizer, llm_loaded_successfully, USE_LLM_4BIT_QUANTIZATION, device
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig # Importações aqui
        import torch

        # --- MODELO LLAMA 3 INSTRUCT ---
        llm_model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
        # -----------------------------
        #print(f"Info: Tentando carregar LLM: {llm_model_id}")

        try:
            #print(f"Info: Carregando tokenizer para {llm_model_id}...")
            current_tokenizer = AutoTokenizer.from_pretrained(
                llm_model_id,
                trust_remote_code=True
            )
            if current_tokenizer.pad_token is None:
                current_tokenizer.pad_token = current_tokenizer.eos_token
                #print(f"Info: pad_token do tokenizer ('{current_tokenizer.pad_token}') foi definido como eos_token ('{current_tokenizer.eos_token}').")
            llm_tokenizer = current_tokenizer


            model_load_kwargs = {
                "trust_remote_code": True,
                "device_map": "auto",
            }

            if USE_LLM_4BIT_QUANTIZATION:
                #print(f"Info: Configurando quantização 4-bit para {llm_model_id}...")
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                )
                model_load_kwargs["quantization_config"] = bnb_config
                model_load_kwargs["torch_dtype"] = torch.bfloat16
            else:
                #print(f"Info: Carregando {llm_model_id} SEM quantização 4-bit (usará MUITA VRAM)...")
                model_load_kwargs["torch_dtype"] = torch.bfloat16

            print(f"Info: Carregando pesos do modelo {llm_model_id} (pode levar alguns minutos)...")
            current_llm_model = AutoModelForCausalLM.from_pretrained(
                llm_model_id,
                **model_load_kwargs
            )
            llm_model = current_llm_model
            print(f"Info: Modelo LLM {llm_model_id} carregado com sucesso!")
            llm_loaded_successfully = True
            return True
        except Exception as e:
            print(f"ERRO FATAL ao carregar o modelo LLM ({llm_model_id}): {e}")
            import traceback
            traceback.print_exc()
            llm_loaded_successfully = False
            llm_model = None
            llm_tokenizer = None
            return False

    # --- Carregar LLM inicialmente ---
    if not load_llm():
        print("ALERTA CRÍTICO: Falha no carregamento inicial do LLM. A geração de texto não funcionará.")

    # --- Configuração do Stable Diffusion XL (Base + Refiner) ---
    if cuda_available:
        from diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline # Importa aqui

        sdxl_base_model_id = "stabilityai/stable-diffusion-xl-base-1.0"
        sdxl_refiner_model_id = "stabilityai/stable-diffusion-xl-refiner-1.0"
        try:
            #print(f"\nInfo: Carregando modelo SDXL Base: {sdxl_base_model_id}...")
            base_pipe = StableDiffusionXLPipeline.from_pretrained(
                sdxl_base_model_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
            )
            #print("Info: Habilitando CPU offload para o modelo SDXL Base...")
            base_pipe.enable_model_cpu_offload()
            #print(f"Info: Modelo SDXL Base carregado com offload para CPU.")

            #print(f"\nInfo: Carregando modelo SDXL Refiner: {sdxl_refiner_model_id}...")
            refiner_pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(
                sdxl_refiner_model_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True,
                text_encoder_2=base_pipe.text_encoder_2, vae=base_pipe.vae
            )
            #print("Info: Habilitando CPU offload para o modelo SDXL Refiner...")
            refiner_pipe.enable_model_cpu_offload()
            #print(f"Info: Modelo SDXL Refiner carregado com offload para CPU.")
            sdxl_loaded_successfully = True
        except Exception as e:
            #print(f"ERRO ao carregar os modelos SDXL: {e}")
            sdxl_loaded_successfully = False; base_pipe, refiner_pipe = None, None
            gc.collect()
            if torch.cuda.is_available(): torch.cuda.empty_cache()
    else:
        #print("\nAVISO: GPU não disponível. Modelos SDXL não serão carregados.")
        sdxl_loaded_successfully = False

    # Mensagens de status finais
    if llm_loaded_successfully: print("\n>>> Modelo de TEXTO (LLM - Llama 3) pronto. <<<")
    else: print("\n>>> Modelo de TEXTO (LLM - Llama 3) NÃO FOI CARREGADO. <<<")

    if sdxl_loaded_successfully: print(">>> Modelos de IMAGEM (SDXL) prontos. <<<")
    elif cuda_available: print(">>> Modelos de IMAGEM (SDXL) NÃO FORAM CARREGADOS. <<<")
    else: print(">>> Modelos de IMAGEM (SDXL) não tentados (sem GPU). <<<")

elif 'environment_ok' not in globals():
    print("ERRO: Célula 2 (Verificação de Ambiente) não executada. Execute-a primeiro.")
else:
    print("ERRO: Ambiente não configurado corretamente (Célula 2 falhou). Modelos não podem ser carregados.")

#@title 4. Funções de Geração (Texto e Imagem)
import gc

if 'environment_ok' in globals() and environment_ok:
    def generate_llm_image_prompt(topic, tone, objective, llm_model_ref, llm_tokenizer_ref):

        if llm_model_ref is None or llm_tokenizer_ref is None: print("ERRO: LLM ausente para prompt de imagem."); return None
        is_product_focused = any(kw in topic.lower() for kw in ["produto", "tênis", "sapato", "perfume", "creme", "lançamento de", "coleção", "item", "peça"])
        style_instruction = "Priorize um estilo de 'fotografia de produto limpa e profissional' (clean, professional product photography) com boa iluminação de estúdio, fundo neutro ou relevante ao produto. Foque no objeto principal. "
        if not is_product_focused: style_instruction = "Crie um prompt para uma imagem conceitual ou representativa que capture a essência do tópico. Pode ser mais abstrato ou simbólico, mas visualmente atraente. "
        image_prompt_messages = [
            {"role": "system", "content": (
                "Você é um especialista em criar prompts detalhados e eficazes para modelos de geração de imagem (Stable Diffusion). "
                "O prompt deve ser em INGLÊS, focado em uma única cena ou objeto, e descrever visualmente o conceito ou produto. "
                f"{style_instruction}"
                "Inclua detalhes sobre iluminação e composição. Máximo 60-70 palavras. APENAS o prompt, sem introduções."
            )},
            {"role": "user", "content": (f"Crie um prompt de imagem em INGLÊS para: '{topic}'. Tom geral do conteúdo: '{tone}'. Objetivo: '{objective}'. Siga a orientação de estilo.")}
        ]
        try:
            inputs = llm_tokenizer_ref.apply_chat_template(image_prompt_messages, add_generation_prompt=True, return_tensors="pt").to(llm_model_ref.device)
            with torch.no_grad(): outputs = llm_model_ref.generate(inputs, max_new_tokens=75, temperature=0.4, top_p=0.9, do_sample=True, eos_token_id=[llm_tokenizer_ref.eos_token_id, llm_tokenizer_ref.convert_tokens_to_ids("<|eot_id|>")], pad_token_id=llm_tokenizer_ref.pad_token_id)
            text = llm_tokenizer_ref.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()
            text = re.sub(r"^(Okay, here's an image prompt:|Here's an image prompt:|Image prompt:|Prompt:)\s*", "", text, flags=re.IGNORECASE).strip()
            encoded = llm_tokenizer_ref.encode(text, add_special_tokens=False);
            if len(encoded) > 77: text = llm_tokenizer_ref.decode(encoded[:77], skip_special_tokens=True, clean_up_tokenization_spaces=True).strip()
            if '.' in text and not text.endswith('.'): text = text.rsplit('.', 1)[0] + '.'
            elif text and not text.endswith(('.', '!', '?')): text += "."
            print(f"Debug LLM Image Prompt: Gerado '{text}'"); return text if text else None
        except Exception as e: print(f"Erro ao gerar prompt de imagem com LLM: {e}"); return None

    # função que gera os textos criativos com llama
    def generate_creative_text(content_type, topic, audience, tone, objective, keywords, cta, length_limit, include_image_desc=False):
        global llm_model, llm_tokenizer, llm_loaded_successfully
        if not llm_loaded_successfully or llm_model is None or llm_tokenizer is None:
            if 'load_llm' in globals() and callable(globals()['load_llm']):
                print("Info: LLM não carregado. Tentando recarregar...")
                if not globals()['load_llm'](): print("ERRO: Falha ao recarregar LLM."); return None, None
            else: print("ERRO: LLM não carregado e sem recarregador."); return None, None

        cleaned_topic_for_name = re.sub(r"^(lançamento de um novo|descubra o nosso|apresentando o|conheça o)\s+", "", topic, flags=re.IGNORECASE)
        name_in_quotes_match = re.search(r"'(.*?)'", cleaned_topic_for_name)
        if name_in_quotes_match: product_name_for_prompt = name_in_quotes_match.group(1).strip()
        else: product_name_for_prompt = " ".join(cleaned_topic_for_name.split()[:4]).split(',')[0].strip()
        if not product_name_for_prompt or len(product_name_for_prompt) < 3: product_name_for_prompt = "este produto"

        specific_instructions = ""; final_cta = cta; use_hashtags_in_body = True; response_format_hint = ""; max_tokens_for_type = 300

        if "E-mail Marketing" in content_type:
            specific_instructions = ("Você é um copywriter especialista em e-mail marketing persuasivo. "
                                     "Crie um e-mail em Português do Brasil, sem erros ortograficos. Estruture com uma linha de assunto (Subject:) impactante e clara, seguida pelo corpo do e-mail. "
                                     "Use parágrafos curtos e linguagem que leve à ação. A CTA deve ser proeminente. Não use hashtags no corpo do e-mail.")
            use_hashtags_in_body = False; max_tokens_for_type = 450
            response_format_hint = "Formato esperado: Subject: [Seu Assunto Aqui]\n\n[Corpo do E-mail...]"
        elif "Descrição de Produto (e-commerce)" in content_type:
            specific_instructions = ("Você é um especialista em criar descrições de produtos para e-commerce que vendem, **obrigatoriamente em Português do Brasil, que seja coerente e sem erros ortograficos**. "
                                     "Destaque benefícios, características únicas (com detalhes técnicos se aplicável) e diferenciais. "
                                     "Use linguagem clara, parágrafos curtos ou bullet points (usando '-' ou '*') para fácil leitura. O objetivo é informar e persuadir.")
            use_hashtags_in_body = False; max_tokens_for_type = 500
        elif "Tweet (X)" in content_type:
            specific_instructions = ("Você é um especialista em criar tweets (posts para X) curtos, virais e que geram engajamento, **em Português do Brasil**, que seja coerente e sem erros ortograficos. "
                                     "Respeite estritamente o limite de 280 caracteres. Use 2-3 hashtags altamente relevantes e concisas. "
                                     "Emojis são bem-vindos para adicionar personalidade, se alinhado com o tom.")
            max_tokens_for_type = 120
        elif "Post para Facebook" in content_type:
            specific_instructions = (f"Você é um redator de marketing especialista em criar posts para Facebook que são informativos e engajadores, **em Português do Brasil**, que seja coerente e sem erros ortograficos."
                                     "Posts no Facebook podem ser um pouco mais longos que no Instagram, permitindo mais detalhes, storytelling ou listas. "
                                     "Use parágrafos bem estruturados, emojis se apropriado, e 3-5 hashtags relevantes.")
            max_tokens_for_type = 400
        elif "Roteiro Curto para Reels/TikTok (ideia)" in content_type:
            specific_instructions = ("Você é um criador de conteúdo especialista em desenvolver ideias e estruturas para roteiros de vídeos curtos (Reels/TikTok), **em Português do Brasil**, que seja coerente e sem erros ortograficos. "
                                     "Forneça uma estrutura de roteiro com 3 a 5 cenas curtas ou momentos chave. "
                                     "Para cada cena, sugira: (1) Descrição Visual Curta, (2) Narração/Diálogo (se houver), (3) Texto na Tela (opcional, curto e impactante). "
                                     "O objetivo é um vídeo dinâmico e cativante. Sugira 2-3 hashtags relevantes para a plataforma.")
            response_format_hint = "Formato esperado:\nROTEIRO:\nCENA 1:\nVisual: [descrição]\nNarração: [texto]\nTexto na Tela: [texto]\n\nCENA 2:\nVisual: [descrição]\nNarração: [texto]\nTexto na Tela: [texto]\n(e assim por diante)"
            max_tokens_for_type = 500 # Dar espaço para a estrutura do roteiro
            use_hashtags_in_body = True # Hashtags são parte da sugestão do roteiro
        else: # Padrão para Instagram, Legenda LinkedIn
            specific_instructions = (f"Você é um redator de marketing especialista em criar posts para {content_type} que são concisos, envolventes e altamente eficazes, **em Português do Brasil**, que seja coerente e sem erros ortograficos. "
                                     "Use parágrafos curtos. Emojis são bem-vindos se alinhados ao tom. Use 3-5 hashtags relevantes.")

        system_prompt_content = (
            f"{specific_instructions} "
            f"Seu público é '{audience}'. O tom OBRIGATÓRIO é '{tone}'. O objetivo principal é '{objective}'. "
            f"O texto deve respeitar o limite de '{length_limit}' e focar nos benefícios de '{product_name_for_prompt}'. "
            "Incorpore as palavras-chave fornecidas: '{keywords}'. " # Tornando mais explícito o uso de keywords
            f"A peça de conteúdo DEVE terminar EXATAMENTE com a Chamada Para Ação (CTA) fornecida: '{final_cta}'. "
            "Não adicione nenhuma introdução (como 'Aqui está:'), explicações, notas ou qualquer texto além do solicitado e da CTA exata. "
            f"Responda apenas com o {content_type} solicitado. {response_format_hint}"
        )
        user_prompt_content = (
            f"Crie um {content_type} impactante para o produto/serviço: '{topic}'. "
            f"Lembre-se das palavras-chave, do tom '{tone}', e que deve terminar com a CTA: '{final_cta}'. "
            "Gere a resposta integralmente em Português do Brasil."
        )

        messages = [{"role": "system", "content": system_prompt_content}, {"role": "user", "content": user_prompt_content}]
        inputs = llm_tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(llm_model.device)
        generation_args = {"max_new_tokens": max_tokens_for_type, "temperature": 0.6, "top_p": 0.9, "do_sample": True,
                           "eos_token_id": [llm_tokenizer.eos_token_id, llm_tokenizer.convert_tokens_to_ids("<|eot_id|>")],
                           "pad_token_id": llm_tokenizer.pad_token_id, "repetition_penalty": 1.15, "no_repeat_ngram_size": 3}
        try:
            with torch.no_grad(): outputs = llm_model.generate(inputs, **generation_args)
            response_text = llm_tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True).strip()
        except Exception as e: print(f"Erro LLM: {e}"); return None, None

        main_text_final = response_text
        phrases_to_clean = ["Aqui está o post:", "Aqui está o seu e-mail:", "Subject:", "ASSUNTO CURTO E IMPACTANTE", "CORPO DO E-MAIL", "Aqui está a descrição do produto:", "Aqui está o tweet:", "Roteiro:", "CENA 1:", "Aqui está uma sugestão:", "Claro, aqui está:", "Post:", "Texto do post:", "utzer_}riba_"]
        for phrase in phrases_to_clean: # Limpeza mais genérica de prefixos
            if main_text_final.lower().startswith(phrase.lower()):
                main_text_final = main_text_final[len(phrase):].lstrip(":\n ").strip()

        # Lógica de CTA
        cta_to_add_to_text = final_cta if use_hashtags_in_body else re.sub(r"#\w+\s*", "", final_cta).strip()
        # Remove qualquer ocorrência da CTA (com ou sem hashtags) do corpo, exceto se já for o final exato
        temp_text_no_hash = re.sub(r"#\w+\s*", "", main_text_final).strip().lower()
        cta_to_check_no_hash = re.sub(r"#\w+\s*", "", cta_to_add_to_text).strip().lower()

        if not temp_text_no_hash.endswith(cta_to_check_no_hash):
            # Remove CTAs genéricas ou a CTA específica se aparecer no meio
            generic_ctas_pattern = r"(visite agora.*?#\w+|explore agora.*?#\w+|clique aqui.*?\[link\]|saiba mais aqui:|compre j[áaAaÁ] agora\!*)"
            main_text_final = re.sub(generic_ctas_pattern, "", main_text_final, flags=re.IGNORECASE).strip()

            # Remove a CTA específica se ela estiver no meio do texto
            # Escapa a CTA para regex e permite pontuação opcional ao redor
            cta_pattern_middle = r"[\s\.,;:!?]*" + re.escape(cta_to_add_to_text) + r"[\s\.,;:!?]+"
            main_text_final = re.sub(cta_pattern_middle, " ", main_text_final, flags=re.IGNORECASE).strip() # Substitui por espaço para evitar junção de palavras

            main_text_final = main_text_final.strip(" .,;:!?") # Limpa pontuação solta no final
            if main_text_final and cta_to_add_to_text: # Adiciona se o texto não estiver vazio e a cta também não
                # Adiciona um ponto se necessário antes da CTA
                if not re.search(r"[.!?]$", main_text_final): main_text_final += "."
                main_text_final += " " + cta_to_add_to_text
            elif cta_to_add_to_text: # Se o texto principal ficou vazio
                main_text_final = cta_to_add_to_text

        main_text_final = main_text_final.strip()
        if not use_hashtags_in_body: # Garante que não haja hashtags se não forem permitidas
            main_text_final = re.sub(r"#\w+\s*", "", main_text_final).strip()

        # Garante pontuação final se o texto não estiver vazio e não terminar com pontuação ou a CTA
        if main_text_final and not re.search(r"[.!?]$", main_text_final) and not main_text_final.endswith(cta_to_add_to_text):
            main_text_final += "."
        main_text_final = re.sub(r"\s+([.!?])$", r"\1", main_text_final) # Limpa espaços antes da pontuação final

        image_description_en = None
        if include_image_desc:
            print("Info: Solicitando ao LLM para gerar descrição da imagem (prompt em inglês)...")
            image_description_en = generate_llm_image_prompt(topic, tone, objective, llm_model, llm_tokenizer)
            if not image_description_en:
                print("AVISO: LLM não conseguiu gerar descrição para imagem. Usando fallback.")
                item_name_for_fallback = product_name_for_prompt if product_name_for_prompt != "este produto" else topic
                image_description_en = f"Professional product photography of '{item_name_for_fallback}', {tone} style, clean background, high quality."

        return main_text_final, image_description_en

    def generate_associated_image_sdxl(prompt_text_en, negative_prompt="text, watermark, signature, username, words, letters, blurry, low quality, ugly, deformed, worst hands, bad anatomy, extra limbs, disfigured, bad art, poorly drawn, unrealistic, CGI, fake, illustration, cartoon, painting, drawing, low-resolution, artifacts, noise, distorted, weird colors, overexposed, underexposed, oversaturated, grainy, people, person, man, woman, foot, feet, multiple items unless specified, text logos, brand names unless specified in prompt, tiling, out of frame, body out of frame, cut off", num_inference_steps_base=25, num_inference_steps_refiner=20, guidance_scale=7.0, high_noise_frac=0.8, width=1024, height=1024):
        global base_pipe, refiner_pipe, sdxl_loaded_successfully
        if not sdxl_loaded_successfully or base_pipe is None or refiner_pipe is None: print("ERRO: Modelos SDXL não carregados."); return None
        if not prompt_text_en or len(prompt_text_en) < 10: print(f"ERRO: Prompt de imagem inválido: '{prompt_text_en}'."); return None
        try:
            print(f"Debug SDXL: Offload ativo base? {hasattr(base_pipe, '_hf_hook') and base_pipe._hf_hook is not None}")
            print(f"Debug SDXL: Offload ativo refiner? {hasattr(refiner_pipe, '_hf_hook') and refiner_pipe._hf_hook is not None}")
            with torch.no_grad():
                print("Debug SDXL: Iniciando base_pipe..."); latents = base_pipe(prompt=prompt_text_en, negative_prompt=negative_prompt, width=width, height=height, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps_base, denoising_end=high_noise_frac, output_type="latent").images
                print("Debug SDXL: base_pipe concluído. Limpando cache CUDA..."); torch.cuda.empty_cache()
                print("Debug SDXL: Iniciando refiner_pipe..."); refined_image = refiner_pipe(prompt=prompt_text_en, negative_prompt=negative_prompt, image=latents, num_inference_steps=num_inference_steps_refiner, denoising_start=high_noise_frac, guidance_scale=guidance_scale).images[0]
                print("Debug SDXL: refiner_pipe concluído.")
            return refined_image
        except Exception as e:
            print(f"Erro SDXL: {e}")
            if "out of memory" in str(e).lower() and torch.cuda.is_available(): print("ERRO DE MEMÓRIA GPU (SDXL).")
            gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None; return None

    print("Funções de geração definidas (v17 - Prompts Especializados para Todos os Tipos).")
else:
    print("ERRO: Ambiente não configurado (Célula 2). Funções não definidas.")

#@title 5. Interface do Usuário e Execução da Geração
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output, Image as IPImage
import time
import gc

if 'environment_ok' in globals() and environment_ok:
    # Verificação inicial do LLM
    if ('llm_loaded_successfully' not in globals() or not llm_loaded_successfully) and \
       ('llm_model' not in globals() or llm_model is None):
        if 'load_llm' in globals() and callable(globals()['load_llm']):
            # print("Info: LLM não carregado, tentando carregar...")
            if not globals()['load_llm'](): # load_llm() em si já foi ajustada para ser menos verbosa
                 print(f"ERRO CRÍTICO: Não foi possível carregar o LLM na inicialização da UI.")
            # else:
                 # print("Info: LLM carregado na inicialização da UI.")
        else:
            print("ERRO FATAL: Função load_llm() não encontrada.")

    display(HTML("<div class='creative-generator-interface'>")) # Classe principal para estilização

    # Avisos iniciais
    if 'llm_loaded_successfully' in globals() and not llm_loaded_successfully and not ('llm_was_unloaded_for_sdxl' in globals() and llm_was_unloaded_for_sdxl):
        display(HTML(f"<div class='error-message-box initial-error'>ERRO: LLM não carregado (ver Célula 3 ou saída acima).</div>"))
    else:
        if cuda_available and ('sdxl_loaded_successfully' not in globals() or not sdxl_loaded_successfully):
             print(f"AVISO: Modelos SDXL não carregaram. Geração de imagem indisponível.")
        elif not cuda_available:
            print(f"AVISO: GPU não disponível. Geração de imagem indisponível.")

    widget_style = {'description_width': 'initial'}
    widget_layout_full = widgets.Layout(width='98%', margin='12px 1%')
    widget_layout_70 = widgets.Layout(width='70%', margin='12px 1%')
    button_layout = widgets.Layout(width='auto', min_width='200px', margin='30px auto 20px auto', display='flex')

    content_type_widget = widgets.Dropdown(options=['Post para Instagram', 'Descrição de Produto (e-commerce)', 'E-mail Marketing (curto e impactante)', 'Legenda para LinkedIn'], value='Post para Instagram', description='Tipo de Conteúdo:', style=widget_style, layout=widget_layout_full)
    topic_widget = widgets.Textarea(value='Lançamento de um novo tênis de corrida feito com 80% de materiais reciclados do oceano, ultra leve e com design moderno.', description='Tema/Produto/Serviço:', style=widget_style, layout=widgets.Layout(width='98%', height='70px', margin='12px 1%'))
    audience_widget = widgets.Text(value='Jovens adultos (18-35 anos), ecologicamente conscientes, ativos nas redes sociais, interessados em esportes e sustentabilidade.', description='Público-Alvo:', style=widget_style, layout=widget_layout_full)
    tone_widget = widgets.Dropdown(options=['Inspirador e motivacional', 'Entusiasmado e energético', 'Engraçado e informal', 'Profissional e informativo', 'Sofisticado e elegante', 'Descontraído e amigável', 'Urgente e direto', 'Consciente e sério'], value='Entusiasmado e energético', description='Tom de Voz:', style=widget_style, layout=widget_layout_full)
    objective_widget = widgets.Textarea(value='Gerar buzz, destacar os benefícios ecológicos e de performance, direcionar para a página de pré-venda.', description='Objetivo Principal:', style=widget_style, layout=widgets.Layout(width='98%', height='50px', margin='12px 1%'))
    keywords_widget = widgets.Text(value='sustentável, reciclado, oceano, performance, conforto, design inovador, pré-venda, edição limitada', description='Palavras-chave (separadas por vírgula):', style=widget_style, layout=widget_layout_full)
    cta_widget = widgets.Textarea(value='Garanta o seu na pré-venda! Clique no link da bio e faça parte da mudança. #EcoRunner #CorraPeloPlaneta', description='Chamada para Ação (CTA) e Hashtags:', style=widget_style, layout=widgets.Layout(width='98%', height='50px', margin='12px 1%'))
    length_widget = widgets.Dropdown(options=['Muito Curto (1-2 frases, ideal para Tweet)', 'Curto (2-4 frases)', 'Médio (4-6 frases)', 'Longo (6-8 frases, se aplicável)'], value='Curto (2-4 frases)', description='Extensão do Texto:', style=widget_style, layout=widget_layout_70)
    include_image_widget = widgets.Checkbox(value=True, description='Gerar Imagem Associada', indent=False, style=widget_style, layout=widget_layout_full)
    image_resolution_options = {"Padrão (1024px)": (1024, 1024), "Média (768px)": (768, 768), "Baixa (512px)": (512, 512)}
    image_resolution_widget = widgets.Dropdown(options=list(image_resolution_options.keys()), value="Padrão (1024px)", description='Resolução Imagem:', style=widget_style, layout=widget_layout_70, disabled=True)

    generate_button = widgets.Button(description="Gerar Conteúdo", button_style='primary', icon='cogs', layout=button_layout, tooltip="Clique para criar!")
    progress_bar = widgets.IntProgress(value=0, min=0, max=100, description='Aguarde:', bar_style='info', orientation='horizontal', layout=widgets.Layout(width='98%', visibility='hidden', margin='15px 1%'))
    status_message_area = widgets.HTML(value="", layout=widgets.Layout(width='98%', margin='5px 1%'))
    output_area = widgets.Output(layout=widgets.Layout(width='100%', margin_top='25px'))
    llm_was_unloaded_for_sdxl = False

    def on_generate_button_clicked(b):
        global llm_model, llm_tokenizer, llm_loaded_successfully, llm_was_unloaded_for_sdxl, base_pipe, refiner_pipe
        with output_area: clear_output(wait=True) # Limpa apenas a área de output principal

        generate_button.disabled = True; progress_bar.value = 0; progress_bar.layout.visibility = 'visible'
        status_message_area.value = f"<p class='status-info'>Iniciando...</p>" # Mensagem inicial curta
        start_time_total = time.time()

        # --- Lógica de Carregamento/Recarregamento do LLM ---
        if llm_was_unloaded_for_sdxl and (llm_model is None or not llm_loaded_successfully):
            status_message_area.value = f"<p class='status-info'>Preparando modelo de texto...</p>"; progress_bar.value = 5
            if not globals()['load_llm']():
                status_message_area.value = f"<p class='status-error-inline'>ERRO: Falha ao preparar modelo de texto.</p>"; progress_bar.layout.visibility = 'hidden'; generate_button.disabled = False; return
            llm_was_unloaded_for_sdxl = False
        elif llm_model is None or llm_tokenizer is None or not llm_loaded_successfully:
            status_message_area.value = f"<p class='status-error-inline'>ERRO: Modelo de texto não está pronto.</p>"; progress_bar.layout.visibility = 'hidden'; generate_button.disabled = False; return

        progress_bar.value = 10
        # --- Coleta de Inputs ---
        content_type, topic = content_type_widget.value, topic_widget.value; audience, tone = audience_widget.value, tone_widget.value
        objective, keywords = objective_widget.value, keywords_widget.value; cta, length = cta_widget.value, length_widget.value
        include_image = include_image_widget.value; selected_resolution_key = image_resolution_widget.value
        img_w, img_h = image_resolution_options[selected_resolution_key]

        if not topic:
            status_message_area.value = f"<p class='status-error-inline'>ATENÇÃO: Forneça um tema/produto.</p>"; progress_bar.layout.visibility = 'hidden'; generate_button.disabled = False; return

        # --- Geração de Texto ---
        status_message_area.value = f"<p class='status-info'>Gerando texto...</p>"; progress_bar.value = 20
        start_time_text = time.time()
        generated_text, image_desc_en = generate_creative_text(content_type, topic, audience, tone, objective, keywords, cta, length, include_image_desc=include_image)
        end_time_text = time.time(); progress_bar.value = 40

        with output_area: # Resultados são exibidos aqui
            if generated_text:
                generated_text_html = generated_text.replace('\n', '<br>')
                text_html = f"<div class='output-box text-output highlighted-text-output'><h4 class='output-title'>Texto Gerado ({end_time_text - start_time_text:.1f}s):</h4><p>{generated_text_html}</p></div>"
                display(HTML(text_html))
            else:
                display(HTML(f"<div class='output-box error-box'><p class='status-error-inline'>ERRO: Falha ao gerar o texto.</p></div>"))
                status_message_area.value = f"<p class='status-error-inline'>Falha na criação do texto.</p>"; progress_bar.layout.visibility = 'hidden'; generate_button.disabled = False; return

            if include_image and image_desc_en:
                display(HTML(f"<div class='output-box info-box image-prompt-box'><b>Prompt para Imagem (EN):</b><br><i>{image_desc_en}</i></div>")); progress_bar.value = 50
                if 'sdxl_loaded_successfully' in globals() and sdxl_loaded_successfully and cuda_available and base_pipe and refiner_pipe:
                    if llm_model:
                        del llm_model; llm_model = None; llm_loaded_successfully = False; llm_was_unloaded_for_sdxl = True
                        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None

                    status_message_area.value = f"<p class='status-info'>Gerando imagem ({img_w}x{img_h})...</p>"; progress_bar.value = 60
                    start_time_image = time.time()
                    generated_image_pil = generate_associated_image_sdxl(image_desc_en, width=img_w, height=img_h)
                    end_time_image = time.time(); progress_bar.value = 90
                    if generated_image_pil:
                        display(HTML(f"<h4 class='output-title image-title'>Imagem Gerada ({end_time_image - start_time_image:.1f}s):</h4>")); display(generated_image_pil)
                    else:
                        display(HTML(f"<div class='output-box error-box'><p class='status-error-inline'>ERRO: Falha ao gerar a imagem SDXL.</p></div>"))
                        status_message_area.value = f"<p class='status-error-inline'>Falha na criação da imagem.</p>" # Atualiza status
                elif not cuda_available: display(HTML(f"<div class='output-box warning-box'><p>AVISO: Imagem não gerada (GPU não disponível).</p></div>"))
                else: display(HTML(f"<div class='output-box warning-box'><p>AVISO: Imagem não gerada (SDXL não carregado).</p></div>"))
            elif include_image and not image_desc_en:
                 display(HTML(f"<div class='output-box warning-box'><p>AVISO: Não foi possível criar um prompt para a imagem.</p></div>"))

        end_time_total = time.time(); progress_bar.value = 100
        status_message_area.value = f"<p class='status-success'>Concluído em {end_time_total - start_time_total:.1f}s.</p>"
        progress_bar.layout.visibility = 'hidden'; generate_button.disabled = False
        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None

    generate_button.on_click(on_generate_button_clicked)

    #Título e Layout da UI
    title_html = HTML("""
        <div class='ui-title-container'>
            <h1 class='ui-title'>Gerador de Conteúdo Multimodal</h1>
            <p class='ui-subtitle'>Textos com Llama 3 e Imagens com Stable Diffusion XL</p>
        </div>
        <hr class='ui-divider'>
    """)
    display(title_html)

    text_tab_children = [content_type_widget, topic_widget, audience_widget, tone_widget, objective_widget, keywords_widget, cta_widget, length_widget]
    image_tab_children = [include_image_widget, image_resolution_widget]
    text_inputs_vbox = widgets.VBox(text_tab_children, layout=widgets.Layout(padding='10px'))
    image_options_vbox = widgets.VBox(image_tab_children, layout=widgets.Layout(padding='10px'))

    accordion = widgets.Accordion(children=[text_inputs_vbox, image_options_vbox], layout=widgets.Layout(width='100%', margin_bottom='25px'))
    accordion.set_title(0, 'Configurar Texto')
    accordion.set_title(1, 'Configurar Imagem')
    accordion.selected_index = 0

    # --- Estilos CSS ---
    ui_styles_html = HTML("""
    <style>
        /* === Variáveis de Cor Base (Modo Claro por Padrão) === */
        .creative-generator-interface {
            --main-bg-color: #fdfcff; --text-color: #4a235a; --accent-color: #9b59b6;
            --accent-color-light: #d7bde2; --accent-color-dark: #8e44ad;
            --input-bg-color: #ffffff; --input-text-color: #4a235a; --border-color: #e8daef;
            --output-box-bg: #f5eeff; --output-box-text: #4a235a;
            --highlighted-output-bg: #e8daef; --highlighted-output-border: var(--accent-color);
            --info-text-color: #884ea0; --success-text-color: #27ae60;
            --error-text-color: #c0392b; --error-box-bg: #fadbd8; --error-box-text-color: #a93226;
            --warning-box-bg: #fef9e7; --warning-box-text-color: #b7950b;
            --header-bg: #f2e8f9; --header-bg-hover: #e8daef;
            --header-bg-selected: var(--accent-color); --header-text-selected: #ffffff;
            --box-shadow-color: rgba(123, 36, 28, 0.1);
            --button-text-color: #ffffff;
            --button-primary-bg: var(--accent-color); --button-primary-border: var(--accent-color-dark);
            --progress-bar-bg: var(--accent-color-light);
        }
        @media (prefers-color-scheme: dark) {
            .creative-generator-interface {
                --main-bg-color: #2c1f3e; --text-color: #f2e8f9;
                --accent-color: #bb86fc; --accent-color-light: #d7bde2; --accent-color-dark: #a06cd5;
                --input-bg-color: #3e2d54; --input-text-color: #f2e8f9;
                --border-color: #5e4a77; --output-box-bg: #382a4f; --output-box-text: #e8dff5;
                --highlighted-output-bg: #4a3564; --highlighted-output-border: var(--accent-color-light);
                --info-text-color: #d7bde2; --success-text-color: #a3e635;
                --error-text-color: #ff8a80; --error-box-bg: #4b1e22; --error-box-text-color: #ffb3b3;
                --warning-box-bg: #504218; --warning-box-text-color: #fff5cc;
                --header-bg: #4a3564; --header-bg-hover: #5a4574;
                --header-bg-selected: var(--accent-color); --header-text-selected: #1c0f2c;
                --box-shadow-color: rgba(0,0,0,0.35);
                --button-primary-bg: var(--accent-color); --button-primary-border: var(--accent-color-dark);
                --progress-bar-bg: var(--accent-color);
            }
        }
        /* Estilos Gerais */
        .creative-generator-interface { background-color: var(--main-bg-color); color: var(--text-color); padding: 30px; border-radius: 15px; font-family: "Poppins", "Segoe UI", Roboto, Arial, sans-serif; max-width: 850px; margin: 25px auto; box-shadow: 0 10px 25px var(--box-shadow-color); border: 1px solid var(--border-color); }
        .ui-title-container { text-align:center; margin-bottom: 30px; }
        .ui-title { color: var(--text-color); font-weight: 600; font-size: 28px; margin-bottom:10px; letter-spacing: 0.5px; }
        .ui-subtitle { color: var(--info-text-color); font-size: 16px; opacity: 0.95; font-weight: 300;}
        .ui-divider { margin-bottom:30px; border-color: var(--border-color); opacity: 0.6; }
        .creative-generator-interface .p-Accordion-header { background-color: var(--header-bg) !important; color: var(--text-color) !important; opacity: 0.9; border: 1px solid var(--border-color) !important; border-radius: 6px; margin-bottom: 5px; padding: 12px 20px !important; font-size: 1.05em; font-weight: 500; transition: background-color 0.2s ease, opacity 0.2s ease; }
        .creative-generator-interface .p-Accordion-header:hover { background-color: var(--header-bg-hover) !important; opacity: 1; }
        .creative-generator-interface .p-Accordion-header.p-mod-current { background-color: var(--header-bg-selected) !important; color: var(--header-text-selected) !important; border-color: var(--accent-color) !important; opacity: 1; font-weight: 600; margin-bottom: 0; border-radius: 6px 6px 0 0; }
        .creative-generator-interface .p-Accordion-content { border: 1px solid var(--header-bg-selected); border-top: none; padding: 20px; background-color: var(--main-bg-color); border-radius: 0 0 6px 6px; margin-bottom: 15px; }
        .creative-generator-interface .widget-label { font-size: 0.95em; color: var(--text-color) !important; opacity:0.9; margin-bottom: 6px; display:block; font-weight: 500;}
        .creative-generator-interface .widget-dropdown select, .creative-generator-interface .widget-text input, .creative-generator-interface .widget-textarea textarea { font-size: 1em; background-color: var(--input-bg-color) !important; color: var(--input-text-color) !important; border: 1px solid var(--border-color) !important; border-radius: 6px; padding: 10px; width: calc(100% - 22px) !important; box-shadow: inset 0px 1px 3px rgba(0,0,0,0.05); transition: border-color 0.2s ease, box-shadow 0.2s ease; }
        .creative-generator-interface .widget-dropdown, .creative-generator-interface .widget-text, .creative-generator-interface .widget-textarea { margin-bottom: 18px; }
        .creative-generator-interface .widget-checkbox label { color: var(--text-color) !important; font-size: 1em; }
        .creative-generator-interface .widget-checkbox input[type="checkbox"] { margin-right: 10px; transform: scale(1.25); vertical-align: middle; }
        .creative-generator-interface .widget-button button { font-size: 1.1em !important; padding: 12px 28px !important; border-radius: 8px !important; font-weight: 600 !important; color: var(--button-text-color) !important; background-image: linear-gradient(45deg, var(--accent-color-light), var(--accent-color-dark)) !important; border: none !important; transition: opacity 0.2s ease, transform 0.1s ease; box-shadow: 0 4px 8px rgba(0,0,0,0.15); }
        .creative-generator-interface .widget-button button:hover { opacity: 0.88 !important; transform: translateY(-2px); box-shadow: 0 6px 12px rgba(0,0,0,0.2); }
        .creative-generator-interface .widget-button button:active { transform: translateY(0px); box-shadow: inset 0 2px 4px rgba(0,0,0,0.15); }
        .creative-generator-interface .widget-progress-bar { background-color: var(--progress-bar-bg) !important; border-radius: 3px; }
        .creative-generator-interface .widget-progress { border: 1px solid var(--border-color) !important; border-radius: 4px; background-color: var(--input-bg-color); padding: 2px; }
        .creative-generator-interface .widget-progress .widget-label {font-size: 0.9em; opacity: 0.85; margin-bottom:0;}
        .creative-generator-interface .status-info { color: var(--info-text-color); margin-top: 10px; margin-bottom: 10px; font-size:0.95em; text-align: center;}
        .creative-generator-interface .status-success { color: var(--success-text-color); font-weight: 600; margin-top: 12px; margin-bottom: 12px; font-size:1.05em; text-align: center;}
        .creative-generator-interface .status-error-inline { color: var(--error-box-text-color); font-weight: 500; background-color: var(--error-box-bg); padding:10px 12px; border-radius:6px; margin: 10px 0; font-size:1em; display: block; text-align: center; border: 1px solid var(--error-text-color); }
        .creative-generator-interface .output-box { background-color: var(--output-box-bg); color: var(--output-box-text); border: 1px solid var(--border-color); border-left: 6px solid var(--accent-color); padding: 20px; margin: 20px 0; white-space: pre-wrap; font-size: 1em; line-height: 1.7; border-radius: 8px; box-shadow: 0 4px 10px var(--box-shadow-color); }
        .creative-generator-interface .highlighted-text-output { background-color: var(--highlighted-output-bg) !important; border-left-color: var(--highlighted-output-border) !important; box-shadow: 0 5px 15px var(--box-shadow-color); transform: scale(1.01); }
        .creative-generator-interface .output-box.text-output p { font-family: 'Lora', serif; font-size: 1.1em; }
        .creative-generator-interface .output-title { margin-top:0; color: var(--text-color); font-size:1.25em; font-weight: 600; border-bottom: 1px solid var(--border-color); padding-bottom: 10px; margin-bottom:15px; opacity: 0.95; }
        .creative-generator-interface .image-title { margin-top:30px; }
        .creative-generator-interface .error-box { border-left-color: var(--error-text-color); background-color: var(--error-box-bg); color:var(--error-box-text-color);}
        .creative-generator-interface .warning-box { border-left-color: #FFC107; background-color: var(--warning-box-bg); color: var(--warning-box-text-color);}
        .creative-generator-interface .info-box { border-left-color: var(--info-text-color); }
        .creative-generator-interface .image-prompt-box { font-style: italic; opacity: 0.9; font-size: 0.95em;}
    </style>
    """)
    display(ui_styles_html)

    ui_container = widgets.VBox([
        accordion,
        generate_button,
        progress_bar,
        status_message_area,
        output_area
    ])
    display(ui_container)

    def handle_include_image_change(change):
        active = change.get('new', False)
        sdxl_ok = 'sdxl_loaded_successfully' in globals() and globals().get('sdxl_loaded_successfully', False)
        base_pipe_ok = 'base_pipe' in globals() and globals().get('base_pipe') is not None
        refiner_pipe_ok = 'refiner_pipe' in globals() and globals().get('refiner_pipe') is not None
        image_resolution_widget.disabled = not (active and cuda_available and sdxl_ok and base_pipe_ok and refiner_pipe_ok)

    include_image_widget.observe(handle_include_image_change, names='value')
    # Chamada inicial para configurar o estado
    handle_include_image_change({'new': include_image_widget.value, 'name': 'value', 'type': 'change', 'owner': include_image_widget})

    display(HTML("</div>"))
else:
    display(HTML(f"<div class='creative-generator-interface error-state'>"))
    display(HTML("<div class='init-error-box'><h2 class='init-error-title'>ERRO NA INICIALIZAÇÃO</h2>"))
    if 'environment_ok' not in globals(): display(HTML("<p>Célula 2 não executada ou falhou.</p></div></div>"))
    elif not environment_ok: display(HTML(f"<p>Ambiente não configurado (Célula 2 falhou).</p></div></div>"))
    elif 'llm_loaded_successfully' in globals() and not llm_loaded_successfully and not ('llm_was_unloaded_for_sdxl' in globals() and llm_was_unloaded_for_sdxl):
        display(HTML(f"<p>LLM não carregado.</p></div></div>"))
    elif 'llm_model' not in globals() or llm_model is None and not ('llm_was_unloaded_for_sdxl' in globals() and llm_was_unloaded_for_sdxl):
         display(HTML(f"<p>LLM parece não estar carregado.</p></div></div>"))
    else:
        display(HTML("<p>Ocorreu um erro não especificado.</p></div></div>"))